%==============================================================================
%== template for LATEX poster =================================================
%==============================================================================
%
%--A0 beamer slide-------------------------------------------------------------
\documentclass[final]{beamer}
\usepackage{graphicx, color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\usepackage[
  natbib = true,
    backend=bibtex,
    isbn=false,
    url=false,
    doi=false,
    eprint=false,
]{biblatex}
%{\tiny
\bibliography{CT_Skull_Stripping}
%}
\renewcommand*{\bibfont}{\scriptsize}

\usepackage{array}

%\graphicspath{{../maps/}{../}}


\def\newblock{\hskip .11em plus .33em minus .07em} %for natbib and beamer 
\usepackage[orientation=landscape,size=a0,
            scale=1,         % font scale factor
            size=custom,width=140,height=83,
           ]{beamerposter}
\setbeamertemplate{frametitle}{
    \vspace{-5cm}\\
    \insertframetitle
}         
           
\geometry{
  hmargin=2.5cm, % little modification of margins
  vmargin = 0cm,
  head=0cm  
}

\usepackage{multirow}
%
%\usepackage{float}
%\usepackage{subfigure}

%\usepackage{caption}
%\captionsetup{compatibility=false}
%\usepackage{subcaption}

%\usepackage{sidecap}

\usepackage{subfig}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{positioning}

\usepackage{adjustbox}

\usepackage[utf8]{inputenc}
%\usepackage{verbatim}

\linespread{1.15}
%
%==The poster style============================================================
\usetheme{sharelatex}

%==Title, date and authors of the poster=======================================
\title
%[Super Conference, 1 - 10 July 2013, New York, USA] % Conference
{ % Poster title
Validated Automatic Brain Extraction of Head CT Images
}




\author{ % Authors
John Muschelli$^{1}$, Natalie~L.~Ullman$^{2}$, Paul~Vespa$^{3}$, Daniel F. Hanley$^{2}$, Ciprian~M.~Crainiceanu$^{1}$
}

%
%\cortext[cor1]{Principal Corresponding Author}
%\address[jhsph]{Department of Biostatistics, Bloomberg School of Public Health, Johns Hopkins University, Baltimore, MD, USA}
%\address[jhmi]{Department of Neurology, Division of Brain Injury Outcomes,  Johns Hopkins Medical Institutions, Baltimore, MD, USA}
%\address[ucla]{Department of Neurosurgery, David Geffen School of Medicine at UCLA, Los Angeles, CA, USA}



\institute
[Johns Hopkins Bloomberg School of Public Health] % General University
{
%Johns Hopkins Bloomberg School of Public Health
\bf{1} Department of Biostatistics, Bloomberg School of Public Health, Johns Hopkins University, Baltimore, MD, USA \\
\bf{2} Department of Neurology, Division of Brain Injury Outcomes,  Johns Hopkins Medical Institutions, Baltimore, MD, USA\\
\bf{3} Department of Neurosurgery, David Geffen School of Medicine at UCLA, Los Angeles, CA, USA
}
\date{\today}


\usepackage{hyperref}
\begin{document}

\vspace{-4cm}
\newcommand{\stickynote}{\includegraphics[height=1.5em]{blank_sticky_note.png}\;}
\renewcommand{\thesubfigure}{\Alph{subfigure}}


\begin{frame}[fragile]
\vspace{-4cm}


<<label=opts, results='hide', echo=FALSE, message = FALSE, warning=FALSE>>=
library(knitr)
opts_chunk$set(echo=FALSE, prompt=FALSE, message=FALSE, warning=FALSE, comment="", results='hide')
@

<<label=setup, echo=FALSE >>=
rm(list=ls())
library(cttools)
library(fslr)
library(oro.dicom)
library(bitops)
library(arules)
library(plyr)
library(reshape2)
library(ggplot2)
library(matrixStats)
library(gridExtra)
library(qdap)
library(ICC)
options(matlab.path='/Applications/MATLAB_R2013b.app/bin')

# username <- Sys.info()["user"][[1]]
rootdir = path.expand("~/CT_Registration")

ROIformat = FALSE
study = "Original_Images"
if (ROIformat) {
  study = "ROI_images"
}

basedir = file.path(rootdir, "Final_Brain_Seg")
resdir = file.path(basedir, "results")
paperdir = file.path(basedir, "Skull_Strip_Paper")
figdir = file.path(paperdir, "figure")

homedir <- file.path(basedir, study)

rdas = list.files(path=homedir, pattern=".*_CT_.*Header_Info.Rda", 
                  full.names = TRUE, recursive = TRUE)
ostubs = gsub("_Header_Info.Rda", "", rdas, fixed=TRUE)
stubs = gsub("_ungantry", "", ostubs)
stopifnot(all(stubs %in% ostubs))
rdas = rdas[!grepl("antry", rdas)]

stopifnot(!any(grepl("antry", rdas)))
rdas = rdas[grepl("Sorted", rdas)]
rda = data.frame(rda=rdas, stringsAsFactors = FALSE)
rda$id = basename(rda$rda)
rda$id = gsub("(.*)_Header_Info.*", "\\1", rda$id)

fname = file.path(resdir, "Overlap_Statistics.Rda")
load(fname)

img.id = unique(basename(ddf$img))
img.id = nii.stub(img.id)
rda = rda[ rda$id %in% img.id, ]
rownames(rda) = NULL

x = sapply(rda$rda, function(x){
  load(x)
  st = dcmtables[, "0018-0050-SliceThickness"]
  ust = unique(st)
  lust = length(ust)
if (lust > 1){
  print(ust)
}
  lust
})

n.var.slice = sum(x > 1)

proper = function(mystr) {
  x= strsplit(mystr, " ")[[1]]
  paste(toupper(substr(x, 1, 1)), tolower(substring(x, 2)),
        sep= "", collapse=" ")
}
uid = unique(basename(ddf$img))
nscans = length(uid)
num_scans = proper(replace_number(nscans))

pid = gsub("(\\d\\d\\d-(\\d|)\\d\\d\\d).*", "\\1", uid)
pid = unique(pid)
npt = length(pid)
ctr = unique(gsub("(\\d\\d\\d)-.*", "\\1", uid))
n.ctr = length(ctr)

ddf = ddf[ !grepl("refill", ddf$ssimg), ]

cs =  sapply(ddf, class) == "list"
cs = names(cs)[cs]
for (icol in cs){
  ddf[, icol] = unlist(ddf[, icol])
}

d = ddf
d$truevol = d$estvol = NULL
makeint = function(data){
  data$scen = gsub(".*_SS_(.*)_Mask.*", "\\1", data$ssimg )
  data$smooth = !grepl("nopresmooth", data$scen)
  data$smooth = revalue(as.character(data$smooth), 
                     c("TRUE"="Smoothed", "FALSE"="Unsmoothed"))
  data$int = gsub("_nopresmooth", "", data$scen)
  data
}
ddf = makeint(ddf)
ddf$diffvol = (ddf$truevol - ddf$estvol) / 1000
ddf$absdiff = abs(ddf$diffvol)

long = melt(d, id.vars = c("id", "img", "rimg", 
	"ssimg", "pid", "hdr"))

long = makeint(long)
long$id = as.numeric(factor(long$id))

runcols =  c("dice", "jaccard", "sens", "spec", "accur", "absdiff")
rc = runcols[ !runcols %in% c("absdiff")]

wmax = function(x){
	which(x == max(x))
}
x = ddf[ ddf$img == ddf$img[1], ]

res = ddply(ddf, .(img), function(x){
	print(x$id[1])
	xx = sapply(x[, rc], wmax)
	xx = x$scen[xx]
	print(xx)
	names(xx) = rc
	xx
})

results= sapply(res[, rc], table)
maxtab = sapply(results, function(x) {
	names(sort(x, decreasing=TRUE)[1])
})

res = ddply(ddf, .(scen), function(x){
	cmin = colMins(as.matrix(x[, runcols]))
	cmax = colMaxs(as.matrix(x[, runcols]))
	cmean = colMeans(as.matrix(x[, runcols]))
	cmed = colMedians(as.matrix(x[, runcols]))
	xx = data.frame(t(cbind(cmin, cmax, cmean, cmed)))
	xx$run = c("min", "max", "mean", "median")
	xx
})


nospec = long[ long$variable %in% c("accur", "sens"),]

long = long[ long$variable != "jaccard", ]

long$variable = revalue(long$variable, c("sens" = "Sensitivity",
                         "spec" = "Specificity",
                         "accur" = "Accuracy", 
                         "dice" = "Dice Similarity Index"))



@



<<tests>>=

mytest = function(...){
  wilcox.test(...)
}
all.smooth.tests = ddply(long, .(int), function(df){
  p.value= ddply(df, .(variable), function(x){
    stats = lapply(list(median, mean, sd), function(func){
      res = aggregate(value ~ smooth, func, data=x)
    })
    names(stats) = c("median", "mean", "sd")
    cn = stats[[1]]$smooth
    stats = lapply(stats, function(x){
      x$smooth = NULL
      x = c(t(x))
      names(x) = cn
      x
    })
    stats = unlist(stats)
    wt = wilcox.test(value ~ smooth, data=x, paired=TRUE)
    tt = t.test(value ~ smooth, data=x, paired=TRUE)
    return(c(wt.p.value=wt$p.value, tt.p.value = tt$p.value,  stats))
  })
}, .progress="text")


all.int.tests = ddply(long, .(smooth), function(df){
  df = df[df$int %in% c("0.01", "0.1"), ]
  p.value= ddply(df, .(variable), function(x){
    stats = lapply(list(median, mean, sd), function(func){
      res = aggregate(value ~ int, func, data=x)
    })
    names(stats) = c("median", "mean", "sd")
    cn = stats[[1]]$int
    stats = lapply(stats, function(x){
      x$int = NULL
      x = c(t(x))
      names(x) = cn
      x
    })
    stats = unlist(stats)
    wt = wilcox.test(value ~ int, data=x, paired=TRUE)
    tt = t.test(value ~ int, data=x, paired=TRUE)
    return(c(wt.p.value=wt$p.value, tt.p.value = tt$p.value,  stats))
  })
}, .progress="text")

smooth = all.int.tests[ all.int.tests$smooth == "Smoothed",]
pval <- function(pval) {
  x <- ifelse(pval < 0.001, "< 0.001", sprintf("= %03.3f", pval))
	return(x)
}
smooth$wt.p.value = pval(smooth$wt.p.value)
num = sapply(smooth, class) == "numeric"
smooth[, num] = round(smooth[, num], 4)
rownames(smooth) = smooth$variable
smooth$variable = smooth$smooth = NULL
smooth.dice = smooth["Dice Similarity Index",]
@



%==============================================================================
%\begin{multicols}{2}
\begin{minipage}{0.2\linewidth}
\section{Image Processing Pipeline}
\includegraphics[width=\linewidth]{Imaging_Pipeline_Flowchart.pdf}
\section{Sources of Funding}
{\scriptsize
The project described and data used were supported by the NIH grants RO1EB012547, T32AG000247, R01NS046309, RO1NS060910, RO1NS085211, R01NS046309, U01NS080824, U01NS080824 and U01NS062851 and RO1MH095836.
}
\end{minipage}
\begin{minipage}{0.39\linewidth}



<<>>=
fname = file.path(resdir, "Longitudinal_Skull_Strip_Data.Rda")
load(fname)
df = all.df[ grep("SS_0.01", all.df$fname, fixed=TRUE), ]

total.N = nrow(df)


fname = file.path(resdir, "ICC_data.Rda")
load(fname)
npt.icc = smod$ngrps['id']
n.mod = smod$devcomp$dims['n']

num_scans.icc = nrow(ddf) 
fail = total.N - num_scans.icc
pct.fail = round(fail/total.N * 100, 1)

icc.ci = ICCest("id", "truevol", data=ddf)
@



%==============================================================================
%==The poster content==========================================================
%==============================================================================
\section{Goals and Methods}
\begin{multicols}{2}
Systematically analyze the performance of the brain extraction tool (BET) \citep{smith_fast_2002}, a function of the FMRIB software library (FSL) \citep{jenkinson_fsl_2012}, on head CT images of patients with intracranial hemorrhage by varying parameters of BET and the use of smoothing after performing CT-specific preprocessing by:
\begin{itemize}
\item Quantitatively comparing the results to the manual gold standard, and
\item Estimating the performance using the intraclass correlation of serial CT scans.
\end{itemize}


\vfill
\columnbreak


%\section{Methods}


Data were from patients with intracranial hemorrhage from MISTIE (Minimally Invasive Surgery plus recombinant-tissue plasminogen activator for Intracerebral Evacuation) stroke trial centers.
\begin{itemize}
\item Sample compared to gold standard: \Sexpr{num_scans} images from \Sexpr{npt} patients. 
\item Intraclass Correlation Estimate: \Sexpr{num_scans.icc} images from \Sexpr{npt.icc} patients, after excluding \Sexpr{fail} scans for craniotomy or skull stripping failure (\Sexpr{pct.fail}\%).  
\end{itemize}
\end{multicols}
%
%\begin{figure}
%\centering
%\begin{tabular}{m{0.3\linewidth}>{\centering}m{0.3\linewidth}p{0.2\linewidth}}
%Data were from patients with intracranial hemorrhage from MISTIE (Minimally Invasive Surgery plus recombinant-tissue plasminogen activator for Intracerebral Evacuation) stroke trial centers.
%\begin{itemize}
%\item Sample compared to gold standard: \Sexpr{num_scans} images from \Sexpr{npt} patients. 
%\item Intraclass Correlation Estimate: \Sexpr{num_scans.icc} images from \Sexpr{npt.icc} patients, after excluding \Sexpr{fail} scans for craniotomy or skull stripping failure (\Sexpr{pct.fail}\%).  
%\end{itemize} &
%\includegraphics[scale=0.7]{Imaging_Pipeline_Flowchart.pdf} & 
%\multirow{1}{\linewidth}[5\baselineskip]{{\bf Figure: Processing Pipeline.} Each image was thresholded using a $0-100$ Hounsfield units (HU) range. In one variant of the pipeline, data were smoothed using a Gaussian kernel ($\sigma=1$mm) and re-thresholded to $0$-$100$ HU; in the other, data were not smoothed.  BET was applied using 1 of 3 fractional intensity (FI) thresholds: $0.01$, $0.1$, $0.35$ and holes in the brain mask produced by BET were filled. }
%\end{tabular}
%\end{figure}



\section{Measuring and Testing Brain Extraction Performance}

<<check_p>>=
stopifnot(all(all.smooth.tests$wt.p.value < 0.01))
@



<<figcap_CT_Skull_Stripping_Figure2>>=
CT_Skull_Stripping_Figure2 = paste0("{\\bf Performance Metric Distribution for Different Pipelines.} ", 
"Panel~\\protect\\subref*{unsmoothed} displays the boxplots for performance measures when running the pipeline with a different fractional intensity (FI), using smoothed data (top) or unsmoothed data (bottom).  Panel~\\protect\\subref*{smoothed} presents the smoothed data only, rescaled to show discrimination between the different FI.", " Overall, FI of $0.01$ and $0.1$ perform better than $0.35$ in all categories other than specificity.  Using smoothed data improves performance in all performance metrics, markedly when an FI of $0.35$ is used.  Panel~\\protect\\subref*{smoothed} demonstrates that using an FI of $0.01$ on smoothed data has high performance on all measures.  " )
@


\begin{tabular}{cc}
\includegraphics[width=0.46\linewidth]{figure/CT_Skull_Stripping_Figure2.png} &
\includegraphics[width=0.46\linewidth]{figure/CT_Skull_Stripping_Figure2b.png} \\
\end{tabular}
\newline
{\bf Performance Metric Distribution for Different Pipelines.} Panel~A displays the performance for brain extraction for the pipelines, panel~B focuses on only those using smoothed images. 
Using an FI of $0.01$ or $0.1$ performed better than $0.35$.  Using an FI of $0.01$ had a higher median sensitivity ($0.9902$) than an FI of $0.1$ ($0.9891$, $p< 0.001$), lower specificity ($0.998$ vs. $0.998$; $p< 0.001$), and no difference in accuracy ($0.9971$ vs. $0.9971$; $p= 0.039$) or DSI ($0.9892$ vs. $0.9895$).



\section{Smoothing Images can Dramatically Increase Performance}

\begin{figure}[htb]
\begin{tabular}{cc}
	\includegraphics[width=0.35\linewidth]{figure/{101-307_20061110_1638_CT_5_RM_Head_SS_0.01_Mask}.png} &
	\includegraphics[width=0.35\linewidth]{figure/{101-307_20061110_1638_CT_5_RM_Head_SS_0.01_nopresmooth_Mask}.png}  
\end{tabular}
\label{fig:ss_example}
\end{figure}

\vfill

\end{minipage}
\begin{minipage}{0.39\linewidth}

%\columnbreak

%
%\begin{SCfigure}[][h]
%  \subfloat{
%  \label{unsmoothed}
%\includegraphics[width=.33\textwidth]{figure/CT_Skull_Stripping_Figure2.png}
%}
%\hfill
%  \subfloat{
%  \label{smoothed}
%\includegraphics[width=.33\textwidth]{figure/CT_Skull_Stripping_Figure2b.png}
%}
%\caption{\Sexpr{CT_Skull_Stripping_Figure2}}
%\label{fig:metrics}
%\end{SCfigure}

\section{ CT Skull Stripping Leads to Consistent Intracranial Volume Estimates}

\begin{tabular}{cc}
	\includegraphics[width=0.45\linewidth]{../results/Intraclass_Correlation_no_crani_check_fill.png} &
	\includegraphics[width=0.45\linewidth]{../results/Intraclass_Correlation_no_crani_check_day10.png} 
\end{tabular}

{\bf Intracranial Volume (ICV) Estimate over Time.}  Each line represents an individual patient's ICV estimate over time.  The data presented used an FI $= 0.01$ and smoothed data.   The left panel shows all data used to estimate the intraclass correlation coefficient (ICC) of $\Sexpr{round(icc.ci$ICC, 2)}, (95\% CI: \Sexpr{round(icc.ci$LowerCI, 2)}, \Sexpr{round(icc.ci$UpperCI, 2)})$.  

%\input{../demographics_short.tex}
%l

%--References------------------------------------------------------------------
\section{Where does it fail?}
\begin{center}
\begin{tabular}{>{\centering}m{0.25\linewidth}>{\centering}m{0.25\linewidth}>{\centering\arraybackslash}m{0.25\linewidth}}
\includegraphics[width=\linewidth]{../results/Neck_Fail.png} &
\includegraphics[width=\linewidth]{../results/Crani_Fail.png} & 
\includegraphics[width=\linewidth]{../results/Total_Fail.png} \\
Much more area than the brain is imaged &
Patient had a craniotomy & 
CT ventricles are low intensity or enlarged 
\end{tabular}
\end{center}

\section{We have code to do this!}

\begin{tabular}{m{0.5\linewidth} m{0.5\linewidth} }
$\bullet$ R code: \url{http://bit.ly/CTBET_RCODE} & $\bullet$ bash code: \url{http://bit.ly/CTBET_BASH}
\end{tabular}


\section{Conclusions}
\vspace*{-0.5cm}
Smoothing the data using a conservative smoother ($1$mm Gaussian kernel) and using an FI of $0.01$ provides good brain extraction.




%\renewcommand{\bibname}{\chapter{References}}
\section{References}
\setlength\bibitemsep{0pt}
\printbibliography[heading=none]\vspace*{-0.5cm}


%==============================================================================
%==End of content==============================================================
%==============================================================================


%--End of references-----------------------------------------------------------

%\end{multicols}
\end{minipage}
%==============================================================================
\end{frame}
\end{document}

